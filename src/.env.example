# =============================================================================
# WebSocket Server Configuration
# =============================================================================
# Copy this file to .env and customize for your environment
# Priority: Environment variables > .env file > defaults

# =============================================================================
# ENVIRONMENT
# =============================================================================
# Environment name: development, staging, production
# Used for environment-specific behavior and logging
ENVIRONMENT=development

# =============================================================================
# SERVER
# =============================================================================
# WebSocket server listen address
# Format: :PORT or HOST:PORT
# Production: :3002 (internal), proxy handles external access
WS_ADDR=:3002

# NATS server URL
# Production: nats://BACKEND_INTERNAL_IP:4222
# Development: nats://localhost:4222
NATS_URL=nats://localhost:4222

# =============================================================================
# RESOURCE LIMITS
# =============================================================================
# These MUST match your docker-compose resource limits!
# Server uses these for admission control and capacity planning

# CPU limit (cores)
# Example: 1.9 for e2-standard-2 (leaves 0.1 for Promtail)
# Must match docker-compose: deploy.resources.limits.cpus
WS_CPU_LIMIT=1.9

# Memory limit (bytes)
# Example: 7516192768 = 7 GB (leaves 1GB for system + Promtail)
# Must match docker-compose: deploy.resources.limits.memory
# Calculate: GB * 1024 * 1024 * 1024
WS_MEMORY_LIMIT=7516192768

# Maximum concurrent connections
# Production: 7,000 (validated for 99%+ success rate)
# Testing: Set to 8,000-10,000 to find breaking point
# Formula: 7,000 × 0.7MB + 974MB overhead = 5,874MB (81% of 7GB)
WS_MAX_CONNECTIONS=7000

# =============================================================================
# WORKER POOL
# =============================================================================
# Leave at 0 for auto-calculation (recommended)
# Auto formula: CPU_LIMIT * 2 (e.g., 1.9 cores → 4 workers)
#
# Manual sizing formula:
#   workers = max(32, connections/40)
#   For 7K connections: max(32, 7000/40) = 175 → round to power of 2 = 192
#
# Queue size formula: workers * 100
WS_WORKER_POOL_SIZE=192
WS_WORKER_QUEUE_SIZE=19200

# =============================================================================
# RATE LIMITING
# =============================================================================
# Critical for preventing overload and goroutine explosion

# NATS message consumption rate (messages/sec)
# Based on production metrics: 280K users, 40K tx/day peak
# Actual rates: 5 msg/sec (average) to 19.7 msg/sec (high peak)
# Setting: 25 msg/sec provides 25% headroom above peak
WS_MAX_NATS_RATE=25

# Broadcast rate limit (broadcasts/sec)
# Should match NATS rate (1 NATS message = 1 broadcast)
WS_MAX_BROADCAST_RATE=25

# Maximum goroutines (hard limit)
# Formula: ((connections × 2) + workers + 13) × 1.2
# Example: ((7000 × 2) + 192 + 13) × 1.2 = 17,046 → round to 17,500
# Actual at runtime: (7,000 × 2) + 192 + 13 = 14,205 (81% of limit)
WS_MAX_GOROUTINES=17500

# =============================================================================
# SAFETY THRESHOLDS
# =============================================================================
# Emergency brakes to prevent cascading failures

# CPU reject threshold (percentage)
# Above this: Reject new connections with 503
# Prevents new load when system struggling
WS_CPU_REJECT_THRESHOLD=75.0

# CPU pause threshold (percentage)
# Above this: Pause NATS consumption (NAK messages for redelivery)
# Gives system time to recover without dropping data
WS_CPU_PAUSE_THRESHOLD=80.0

# =============================================================================
# JETSTREAM CONFIGURATION
# =============================================================================
# NATS JetStream settings for reliable message delivery

# Stream name (should match publisher)
JS_STREAM_NAME=ODIN_TOKENS

# Consumer name (unique per ws-server instance)
JS_CONSUMER_NAME=ws-server

# Maximum message age in stream
# Old messages automatically deleted
# Recommendation: 30s (sufficient for reconnection scenarios)
JS_STREAM_MAX_AGE=30s

# Maximum messages in stream
# Older messages deleted when limit reached
# 100K messages ≈ 50MB @ 512 bytes/message
JS_STREAM_MAX_MSGS=100000

# Maximum stream size (bytes)
# 50MB = enough for 100K messages @ 512 bytes
JS_STREAM_MAX_BYTES=52428800

# Consumer acknowledgment timeout
# How long JetStream waits for ACK before redelivery
# Match with processing time: 30s allows for retries
JS_CONSUMER_ACK_WAIT=30s

# =============================================================================
# MONITORING
# =============================================================================
# Metrics collection interval
# Balance: More frequent = better visibility, higher CPU cost
# Recommendation: 15s for production
METRICS_INTERVAL=15s

# =============================================================================
# LOGGING
# =============================================================================
# Log level: debug, info, warn, error
# Production: info (balance between visibility and volume)
# Development: debug (verbose)
# Troubleshooting: debug (temporary)
LOG_LEVEL=info

# Log format: json, text, pretty
# Production: json (structured logs for Loki/Grafana)
# Development: pretty (human-readable with colors)
LOG_FORMAT=json

# =============================================================================
# PRODUCTION DEPLOYMENT NOTES
# =============================================================================
#
# 1. INSTANCE SIZING (e2-standard-2: 2 vCPU, 8GB RAM)
#    - WS_CPU_LIMIT=1.9        (reserve 0.1 for Promtail)
#    - WS_MEMORY_LIMIT=7GB     (reserve 1GB for system + Promtail)
#    - WS_MAX_CONNECTIONS=7000 (99%+ success rate validated)
#
# 2. RATE LIMITS
#    - Based on production metrics: 280K users, 40K tx/day peak
#    - WS_MAX_NATS_RATE=25     (25% headroom above peak)
#    - Total throughput: 7,000 × 25 = 175K writes/sec
#
# 3. WORKER POOL
#    - Formula: max(32, connections/40) = 192 workers
#    - Load per worker: (7000 × 25) / 192 = 911 msg/sec (optimal: 300-1,000)
#
# 4. MONITORING
#    - Prometheus metrics: /metrics endpoint
#    - Structured logs: JSON → Promtail → Loki → Grafana
#    - Health checks: /health endpoint (includes resource usage)
#
# 5. SCALING
#    - Vertical: Increase to e2-standard-4 (4 vCPU, 16GB) → 15K connections
#    - Horizontal: Add instances behind load balancer (sticky sessions required)
#
# See: /docs/production/IMPLEMENTATION_PLAN.md for complete architecture
