# =============================================================================
# WebSocket Server Production Configuration
# =============================================================================
# Purpose: Production deployment on GCP e2-standard-4
# Instance: odin-ws-go (4 vCPU, 16GB RAM)
# Configuration: 15K connections @ 1 core (71% memory, avoids lock contention)

ENVIRONMENT=production

# =============================================================================
# SERVER
# =============================================================================
WS_ADDR=:3002
# Production NATS URL (will be substituted by deployment script)
NATS_URL=nats://${BACKEND_INTERNAL_IP}:4222

# =============================================================================
# RESOURCE LIMITS (e2-standard-4 configuration - 15K connections @ 1 core)
# =============================================================================
# Instance: e2-standard-4 (4 vCPU, 16GB RAM = 16,384 MB)
# Target: 15,000 connections @ ~35% CPU, ~71% memory (1 core only)
# GOMAXPROCS: floor(1.0) = 1 core (avoids lock contention, proven stable)
# Strategy: 14.5 GB memory (1.5GB system headroom to prevent memory pressure)
# Note: Multi-core requires lock-free SubscriptionIndex & ReplayBuffers (future work)
# Headroom: 3 cores free (75% CPU), 3.15GB memory (19.7% of total)
WS_CPU_LIMIT=1  # 100% of 1 vCPU → GOMAXPROCS=1 (avoids lock contention in SubscriptionIndex/ReplayBuffers)
WS_MEMORY_LIMIT=15569256448  # 14.5 GB (90% of 16GB, leaves 1.5GB for system - validated)

# Connections: Scaled to 15K (71% memory utilization, 1 CPU core to avoid lock contention)
# Single-threaded execution (GOMAXPROCS=1) avoids lock contention in SubscriptionIndex and ReplayBuffers
# Multi-core scaling requires lock-free data structures (future optimization)
WS_MAX_CONNECTIONS=15000

# =============================================================================
# WORKER POOL - Critical Design Trade-off (Validated over 1+ hour @ 15K connections)
# =============================================================================
# Task submission rate: 25 tasks/sec (1 task per NATS message)
# Each task broadcasts to 15K clients internally (not 15K separate tasks)
# Broadcast duration: ~20ms per broadcast at 15K connections
# Queue capacity: 19,200 slots = 768 seconds of buffer capacity
#
# WHY 192 WORKERS (instead of 256, 320, or 512):
#
# The Critical Trade-off:
#   MORE workers = FASTER broadcasts = BURST traffic = Client disconnections ❌
#   FEWER workers = SLOWER broadcasts = SMOOTH traffic = Stable connections ✅
#
# Test Results Comparison:
#
#   512 workers @ 12K connections:
#     ✅ Worker queue: 0 drops (fast enough to process all broadcasts)
#     ❌ Clients: 969 slow client disconnections (23% failure rate)
#     ❌ Root cause: 2ms bursts overwhelm writePump serial network I/O
#     ❌ Verdict: UNSTABLE - clients can't drain send buffers fast enough
#
#   192 workers @ 12K connections:
#     ✅ Worker queue: ~0 drops (sufficient for message rate)
#     ✅ Clients: 9 timeouts only (99.93% success rate)
#     ✅ Broadcast: 15ms spread naturally paces traffic
#     ✅ Verdict: STABLE - "near perfect" in production testing
#
#   192 workers @ 15K connections (current config):
#     ⚠️  Worker queue: 22,816 drops over 73 minutes (6 drops/sec, 24% drop rate)
#     ✅ Clients: 0 slow client disconnections (99.4% connection stability)
#     ✅ Sustained: 14,917/15,000 connections stable for 1+ hour
#     ✅ NATS redelivery: Dropped broadcasts redelivered (0-30sec delay)
#     💡 Verdict: CONNECTION STABILITY prioritized over broadcast throughput
#
# Why This Trade-off Makes Sense:
#
# 1. Client Perspective (User Experience):
#    - 192 workers: Message delayed 0-30sec (inconvenient but usable)
#    - 512 workers: Connection drops every 2-3 min (completely broken UX)
#    - Choice: Delayed messages >> Disconnections
#
# 2. Architecture Reality:
#    - NATS JetStream redelivers un-ACKed messages (safety net)
#    - Dropped worker tasks → no ACK → automatic redelivery
#    - Client disconnections → complete data loss for that user
#    - Redelivery masks worker drops; nothing masks disconnections
#
# 3. Single-Core Constraint (GOMAXPROCS=1):
#    - 1 CPU core can process ~50 broadcasts/sec (1000ms ÷ 20ms)
#    - Publisher sends 25 broadcasts/sec (should be sufficient)
#    - BUT: JavaScript timer bursts cause spikes (10 msgs in 10ms)
#    - Combined with GC pauses, lock overhead, and 15K goroutine scheduling
#    - Actual capacity: ~19-21 broadcasts/sec (explains 24% drop rate)
#
# 4. Lock Contention Avoidance:
#    - GOMAXPROCS=1 prevents true parallelism (no concurrent lock access)
#    - SubscriptionIndex and ReplayBuffer use mutexes (would contend on multi-core)
#    - Increasing to 2-3 cores would require lock-free data structures
#
# The Path Forward:
#   Option A: Accept 24% drops with NATS redelivery (current - stable connections)
#   Option B: Increase to 2-3 CPU cores + keep 192 workers (eliminates drops)
#   Option C: Reduce to 12K connections @ 192 workers (eliminates drops)
#
# Current Choice: Option A - prioritize connection stability over broadcast speed.
# Future: Option B when lock-free data structures implemented for multi-core.
WS_WORKER_POOL_SIZE=192
WS_WORKER_QUEUE_SIZE=19200

# =============================================================================
# RATE LIMITING (Fixed - based on production metrics)
# =============================================================================
# Production metrics: 280K users, 40K tx/day peak → 5-20 msg/sec actual
# Setting: 25 msg/sec provides 25% headroom above peak
# NOTE: Does NOT scale with connections (1 NATS msg = 1 broadcast to ALL connections)
WS_MAX_NATS_RATE=25
WS_MAX_BROADCAST_RATE=25

# Goroutine limit
# Formula: ((connections × 2) + workers + 13) × 1.2
# = ((15,000 × 2) + 192 + 13) × 1.2 = 36,246 → rounded to 36,500
# Memory cost: 36,500 × 8KB = 292MB (~2.0% of 14.5GB allocated RAM)
WS_MAX_GOROUTINES=36500

# =============================================================================
# SAFETY THRESHOLDS
# =============================================================================
WS_CPU_REJECT_THRESHOLD=75.0
WS_CPU_PAUSE_THRESHOLD=80.0

# =============================================================================
# JETSTREAM
# =============================================================================
JS_STREAM_NAME=ODIN_TOKENS
JS_CONSUMER_NAME=ws-server-dev
JS_STREAM_MAX_AGE=30s
JS_STREAM_MAX_MSGS=100000
JS_STREAM_MAX_BYTES=52428800
JS_CONSUMER_ACK_WAIT=30s

# =============================================================================
# MONITORING
# =============================================================================
METRICS_INTERVAL=15s

# =============================================================================
# LOGGING (Production - structured logs for Loki)
# =============================================================================
LOG_LEVEL=info  # Production log level
LOG_FORMAT=json  # Structured JSON logs for Loki/Grafana
