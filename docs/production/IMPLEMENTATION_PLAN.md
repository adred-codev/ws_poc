# Implementation Plan: Production WebSocket Deployment

## Current State Assessment

### âœ… Already Implemented

**Excellent news**: The WebSocket server (`/src`) is production-ready with:

1. **JetStream Integration** (`server.go:186-327`)
   - Stream creation/management
   - Durable consumer with manual ack
   - Message replay capability
   - Rate limiting built-in

2. **Client Reliability** (`connection.go`)
   - Sequence numbers for message ordering
   - Replay buffer (100 messages per client)
   - Slow client detection & auto-disconnect
   - Connection pooling for efficiency

3. **Resource Management** (`resource_guard.go`)
   - CPU/memory monitoring
   - Dynamic throttling based on load
   - Goroutine limits
   - Emergency brakes (reject at 75% CPU)

4. **Production Features**
   - Structured logging (zerolog â†’ Loki)
   - Prometheus metrics
   - Rate limiting (NATS consumption, broadcasts)
   - Worker pool for message fan-out
   - Health checks
   - Graceful shutdown

### ðŸ”§ Missing Pieces (Minimal Work)

1. **Client Subscription Management**
   - Currently broadcasts to ALL clients
   - Need: Filter by client subscriptions
   - Location: `server.go:818-819` (marked as "future enhancement")
   - Effort: 1-2 days

2. **Backend Integration**
   - Backend needs to publish to NATS/JetStream
   - Effort: 1-2 days

3. **Frontend WebSocket Client**
   - Build subscription protocol
   - Effort: 2-3 days

---

## Decision: Managed NATS (Synadia Cloud)

### Why Managed vs Self-Hosted?

| Factor | Self-Hosted (e2-micro) | Synadia Cloud |
|--------|------------------------|---------------|
| **Cost** | $6/month | $0 (free tier) - $50/month |
| **Setup Time** | 2-4 hours | 5 minutes |
| **Ops Overhead** | Monitor, upgrade, debug | Zero |
| **Reliability** | Single node (no HA) | 3+ nodes, 99.99% SLA |
| **JetStream** | Manual config | Pre-configured |
| **Scaling** | Manual (add nodes) | Automatic |
| **Monitoring** | Setup Prometheus scraping | Built-in dashboards |
| **Multi-Region** | Complex setup | Click a button |

### Recommendation: **Synadia Cloud Free Tier**

**Why perfect for your use case**:
- Free tier: 10GB bandwidth/month, 1GB storage
- Your usage (based on production metrics):
  - **Average day** (1,100 tx): ~450 MB/month
  - **Peak day** (40,000 tx): ~7.5 GB/month sustained
  - Calculation: 0.46 tx/sec Ã— 3.04 events/tx Ã— 500 bytes = ~700 bytes/sec
- **Comfortably within free tier** (using 45-75% of 10GB limit)
- Zero ops overhead
- Production-ready from day 1
- Upgrade to paid tier only if viral growth occurs

**If exceeding free tier**:
- Paid tier: $0.10/GB bandwidth
- Your cost at 2x peak: ~15GB Ã— $0.10 = **$1.50/month**
- Still cheaper than self-hosting ($6/month)

**Verdict**: Use Synadia Cloud, start on free tier

---

## JetStream Configuration

### Current Setup (Already in Code)

Your `server.go` already configures JetStream:

```go
// Stream name: "ODIN_TOKENS" (line 199)
// Subject: "odin.token.>" (line 253)
// Retention: 30 seconds (line 57)
// Max messages: 100,000 (line 58)
// Max bytes: 50MB (line 59)
```

### Recommended JetStream Topics

**Per-Token Updates** (Hierarchical Format: `odin.token.{SYMBOL}.{EVENT_TYPE}`):
```
odin.token.{SYMBOL}.trade        - Trade executed (buy/sell)
odin.token.{SYMBOL}.liquidity    - Liquidity added/removed
odin.token.{SYMBOL}.metadata     - Token name, description, socials updated
odin.token.{SYMBOL}.social       - Comments, reactions, community activity
odin.token.{SYMBOL}.favorites    - User favorited/unfavorited
odin.token.{SYMBOL}.creation     - New token created
odin.token.{SYMBOL}.analytics    - Analytics snapshots
odin.token.{SYMBOL}.balances     - Holder balance changes
```

**Global Topics**:
```
odin.global.trending              - Top 100 trending (every 5 sec)
odin.global.new_listing           - New token created
```

**Why JetStream for this use case**:
1. **Persistence**: Messages survive NATS restart
2. **Replay**: Clients can catch up on missed messages
3. **Exactly-once**: No duplicate messages
4. **Ordering**: Per-subject message ordering guaranteed
5. **Monitoring**: Built-in stream metrics

### JetStream Stream Config

```javascript
// Backend creates stream once (or use Synadia UI)
const stream = {
  name: "ODIN_TOKENS",
  subjects: [
    "odin.token.>",      // All token updates
    "odin.global.>"      // Global updates
  ],
  retention: "limits",   // Discard oldest when limit reached
  max_age: 30_000_000_000, // 30 seconds (nanoseconds)
  max_msgs: 100_000,     // 100k messages max
  max_bytes: 52_428_800, // 50MB max
  storage: "file",       // Persistent storage
  replicas: 1            // Single replica (free tier)
}
```

**Why 30 second retention**:
- Clients should reconnect within 30s
- Longer retention = more storage = higher cost
- 30s covers network blips, not client crashes
- Client crashes â†’ full state reload from REST API

---

## Implementation Tasks

### Phase 1: Infrastructure Setup (Week 1)

**1.1 Sign up for Synadia Cloud** (5 minutes)
- Go to https://www.synadia.com/cloud
- Create free account
- Create JetStream stream: "ODIN_TOKENS"
- Get connection URL + credentials

**1.2 Deploy ws-go to GCP** (2-3 hours)
```bash
# Create 2Ã— e2-small instances
gcloud compute instances create ws-go-1 ws-go-2 \
  --machine-type=e2-small \
  --zone=us-central1-a

# Deploy docker-compose with updated NATS URL
NATS_URL="nats://connect.ngs.global:4222" \
NATS_CREDS="/path/to/synadia.creds" \
docker compose up -d
```

**1.3 Set up monitoring** (2-3 hours)
- Deploy Prometheus, Grafana, Loki on separate e2-small
- Configure service discovery for ws-go instances
- Import Grafana dashboards from `/docs/monitoring/`

**1.4 Configure Load Balancer** (1-2 hours)
```bash
# Create TCP load balancer
gcloud compute forwarding-rules create ws-lb \
  --ports=3004 \
  --backend-service=ws-backend

# Health check
gcloud compute health-checks create http ws-health \
  --port=3002 --path=/health
```

### Phase 2: Code Changes (Week 2)

**2.1 Add Client Subscription Management** (1-2 days)

Add to `connection.go`:
```go
type Client struct {
    // ... existing fields ...
    subscriptions map[string]bool  // NEW: channels client is subscribed to
    subMu         sync.RWMutex     // NEW: protects subscriptions map
}

func (c *Client) subscribe(channels []string) {
    c.subMu.Lock()
    defer c.subMu.Unlock()

    for _, ch := range channels {
        c.subscriptions[ch] = true
    }
}

func (c *Client) isSubscribed(channel string) bool {
    c.subMu.RLock()
    defer c.subMu.RUnlock()
    return c.subscriptions[channel]
}
```

Add to `message.go`:
```go
type SubscribeMessage struct {
    Type     string   `json:"type"`     // "subscribe"
    Channels []string `json:"channels"` // ["odin.token.abc.update", ...]
}

type UnsubscribeMessage struct {
    Type     string   `json:"type"`     // "unsubscribe"
    Channels []string `json:"channels"`
}
```

Update `server.go` NATS handler:
```go
// Current (line 253): broadcasts to ALL clients
// Change to: filter by subscription

sub, err := s.natsJS.Subscribe("odin.token.>", func(msg *nats.Msg) {
    subject := msg.Subject // "odin.token.abc123.update"

    // Count how many clients want this message
    var sentCount int64

    s.clients.Range(func(key, value interface{}) bool {
        client := value.(*Client)

        // Check if client subscribed to this channel
        if client.isSubscribed(subject) {
            select {
            case client.send <- msg.Data:
                sentCount++
            default:
                // Client too slow, already handled by existing code
            }
        }
        return true
    })

    // Metrics
    if sentCount > 0 {
        RecordMessageSent(subject, sentCount)
    }

    msg.Ack()
})
```

**2.2 Update `handleClientMessage`** (1 hour)

In `server.go:815`, implement subscribe/unsubscribe:
```go
func (s *Server) handleClientMessage(client *Client, data []byte) {
    var base struct {
        Type string `json:"type"`
    }
    if err := json.Unmarshal(data, &base); err != nil {
        return
    }

    switch base.Type {
    case "subscribe":
        var msg SubscribeMessage
        if err := json.Unmarshal(data, &msg); err != nil {
            return
        }
        client.subscribe(msg.Channels)
        s.sendSubscriptionConfirmation(client, msg.Channels)

    case "unsubscribe":
        var msg UnsubscribeMessage
        if err := json.Unmarshal(data, &msg); err != nil {
            return
        }
        client.unsubscribe(msg.Channels)

    case "replay":
        // Existing replay logic (already implemented)

    case "heartbeat":
        // Existing heartbeat logic (already implemented)
    }
}
```

**Effort**: ~8 hours total

### Phase 3: Backend Integration (Week 2)

**3.1 Backend Publishes to JetStream**

Install NATS client in backend:
```bash
npm install nats
```

Connect to Synadia Cloud:
```javascript
import { connect, StringCodec } from 'nats';

const nc = await connect({
  servers: 'nats://connect.ngs.global:4222',
  authenticator: credsAuthenticator(await Deno.readFile('./synadia.creds'))
});

const js = nc.jetstream();
const sc = StringCodec();
```

Publish after trade:
```javascript
// After trade completes
const token = await getUpdatedToken(tokenId);

await js.publish(`odin.token.${tokenId}.update`, sc.encode(JSON.stringify({
  id: token.id,
  price: token.price,
  volume: token.volume,
  holder_count: token.holder_count,
  // ... all fields from token.ts
})));
```

**When to publish**:
- After buy/sell trade
- After add/remove liquidity
- After metadata update
- After comment posted
- Scheduled: trending list every 5 seconds

**3.2 Publish Trending List** (cron job)
```javascript
// Every 5 seconds
setInterval(async () => {
  const trending = await getTrendingTokens(100);

  await js.publish('odin.global.trending', sc.encode(JSON.stringify({
    type: 'trending_update',
    data: { top_100: trending }
  })));
}, 5000);
```

**Effort**: ~8 hours total

### Phase 4: Frontend Integration (Week 3)

**4.1 WebSocket Client Library**

Create `lib/websocket-client.ts`:
```typescript
export class OdinWebSocket {
  private ws: WebSocket | null = null;
  private subscriptions = new Set<string>();
  private listeners = new Map<string, Function[]>();

  connect(url: string, token: string) {
    this.ws = new WebSocket(`${url}?auth=${token}`);

    this.ws.onopen = () => {
      console.log('Connected to WebSocket');
      // Resubscribe after reconnect
      if (this.subscriptions.size > 0) {
        this.send({
          type: 'subscribe',
          channels: Array.from(this.subscriptions)
        });
      }
    };

    this.ws.onmessage = (event) => {
      const msg = JSON.parse(event.data);

      // Handle sequence numbers (already in server response)
      if (msg.seq) {
        this.checkSequence(msg.seq);
      }

      // Emit to listeners
      this.emit(msg.type, msg.data);
    };

    this.ws.onclose = () => {
      setTimeout(() => this.connect(url, token), this.getBackoff());
    };
  }

  subscribe(channels: string[]) {
    channels.forEach(ch => this.subscriptions.add(ch));
    this.send({ type: 'subscribe', channels });
  }

  unsubscribe(channels: string[]) {
    channels.forEach(ch => this.subscriptions.delete(ch));
    this.send({ type: 'unsubscribe', channels });
  }

  private checkSequence(seq: number) {
    // If gap detected, request replay (server already supports this!)
    if (this.lastSeq && seq > this.lastSeq + 1) {
      this.send({
        type: 'replay',
        from_seq: this.lastSeq + 1,
        to_seq: seq - 1
      });
    }
    this.lastSeq = seq;
  }
}
```

**4.2 React Hook**

```typescript
export function useTokenWebSocket() {
  const [ws, setWs] = useState<OdinWebSocket | null>(null);

  useEffect(() => {
    const socket = new OdinWebSocket();
    socket.connect(WS_URL, authToken);
    setWs(socket);

    return () => socket.disconnect();
  }, []);

  return ws;
}
```

**4.3 Update Components**

Replace polling with WebSocket subscriptions:
```typescript
// Token List Page
function TokenList() {
  const ws = useTokenWebSocket();
  const [trending, setTrending] = useState<Token[]>([]);

  useEffect(() => {
    if (!ws) return;

    ws.subscribe(['odin.global.trending']);
    ws.on('trending_update', (data) => {
      setTrending(data.top_100);
    });

    return () => ws.unsubscribe(['odin.global.trending']);
  }, [ws]);

  return <div>{/* Render trending */}</div>;
}

// Token Detail Page
function TokenDetail({ tokenId }: { tokenId: string }) {
  const ws = useTokenWebSocket();
  const [token, setToken] = useState<Token | null>(null);

  useEffect(() => {
    if (!ws) return;

    ws.subscribe([`odin.token.${tokenId}.update`]);
    ws.on('token_update', (data) => {
      if (data.id === tokenId) {
        setToken(data);
      }
    });

    return () => ws.unsubscribe([`odin.token.${tokenId}.update`]);
  }, [ws, tokenId]);

  return <div>Price: {token?.price}</div>;
}
```

**Effort**: ~16 hours total

### Phase 5: Testing & Rollout (Week 4-5)

**5.1 Load Testing** (2 days)
```bash
# Test with sustained-load-test.cjs (already exists!)
TARGET_CONNECTIONS=5000 \
DURATION=300 \
WS_URL=wss://staging.example.com/ws \
node scripts/sustained-load-test.cjs
```

**5.2 Alpha Rollout (1 week)**
- Deploy to production
- Enable for 10% of users (feature flag)
- Monitor: connections, message rates, latency, errors
- Collect feedback

**5.3 Beta â†’ GA (1 week)**
- Increase to 50%, then 100%
- Monitor costs, performance
- Disable REST polling

---

## Deployment Configuration

### docker-compose.yml Updates

```yaml
services:
  ws-go:
    environment:
      # Synadia Cloud NATS
      - NATS_URL=nats://connect.ngs.global:4222
      - NATS_CREDS_FILE=/etc/nats/synadia.creds

      # JetStream config (already in code)
      - JS_STREAM_NAME=ODIN_TOKENS
      - JS_CONSUMER_NAME=ws-server
      - JS_STREAM_MAX_AGE=30s
      - JS_STREAM_MAX_MSGS=100000
      - JS_STREAM_MAX_BYTES=52428800

      # Production settings
      - WS_MAX_CONNECTIONS=10000
      - WS_MAX_NATS_RATE=0          # Unlimited (current load is light)
      - WS_MAX_BROADCAST_RATE=0     # Unlimited

    volumes:
      - ./synadia.creds:/etc/nats/synadia.creds:ro
```

### Auto-Scaling (GCP MIG)

```yaml
min_instances: 2
max_instances: 5

scale_up:
  - CPU > 60% for 2 min
  - Connections > 8000

scale_down:
  - CPU < 20% for 10 min
  - Connections < 4000
```

---

## Cost Summary

| Component | Type | Monthly Cost |
|-----------|------|--------------|
| ws-go (2-3 instances) | e2-small | $24-36 |
| NATS (Synadia Cloud) | Free tier | $0 |
| Load Balancer | TCP LB | $18 |
| Monitoring | e2-small | $12 |
| Network egress (compressed) | ~10 TB | $900 |
| **TOTAL** | | **$954-966** |

**At scale (40k concurrent)**:
- ws-go (4 instances): $48
- NATS: $0-10 (may exceed free tier)
- Network: $1,800
- **Total**: ~$1,878

---

## Timeline Summary

| Week | Phase | Effort | Outcome |
|------|-------|--------|---------|
| 1 | Infrastructure | 8-16 hours | NATS + ws-go deployed |
| 2 | Code changes | 16-24 hours | Subscriptions working |
| 3 | Frontend | 16-24 hours | WebSocket client ready |
| 4 | Testing | 16 hours | Load tested, bugs fixed |
| 5 | Alpha rollout | 1 week | 10% users live |
| 6 | Beta â†’ GA | 1 week | 100% users live |

**Total timeline**: 6 weeks
**Total engineering effort**: ~60-80 hours

---

## Risk Mitigation

### Risk: Synadia Cloud free tier exceeded

**Symptoms**: Bandwidth > 10GB/month

**Mitigation**:
1. Enable WebSocket compression (60% reduction) - CRITICAL
2. Upgrade to paid tier ($0.10/GB = $2-5/month)
3. If costs spiral: Self-host NATS cluster (3Ã— e2-micro = $18/month)

### Risk: More concurrent users than expected

**Symptoms**: Need >5 instances

**Mitigation**:
1. Increase MIG max instances to 10 (50k capacity)
2. Enable connection limits per user (max 3 devices)
3. Use larger instances (n2-standard-2 with 10 Gbps network)

### Risk: Backend can't keep up with JetStream publishing

**Symptoms**: Publish latency >100ms

**Mitigation**:
1. Batch multiple updates into single message
2. Use async publishing (fire and forget)
3. Add dedicated publisher service

---

## Future Enhancements

### Adaptive Throttling for Slow Clients

**Problem**: Current implementation disconnects slow clients after 3 consecutive send failures. While effective, this can be disruptive for clients experiencing temporary network issues or device constraints.

**Solution**: Implement adaptive per-client throttling instead of immediate disconnection.

#### Implementation Overview

**Core Concept**: When a client becomes slow, reduce their message rate instead of disconnecting them.

```go
type Client struct {
    // ... existing fields ...

    // Adaptive throttling fields
    throttleRate     float64 // 0.0-1.0 (percentage of messages to send)
    throttleLevel    int32   // 0=none, 1=light, 2=medium, 3=heavy
    lastThrottleTime time.Time
}

// Throttle levels
const (
    ThrottleNone   = 0  // 100% of messages
    ThrottleLight  = 1  // 75% of messages
    ThrottleMedium = 2  // 50% of messages
    ThrottleHeavy  = 3  // 25% of messages
)
```

#### Throttling Logic

```go
func (s *Server) broadcast(message []byte) {
    s.clients.Range(func(key, value interface{}) bool {
        client := key.(*Client)

        // Get current throttle level
        throttleLevel := atomic.LoadInt32(&client.throttleLevel)

        // Determine if we should send this message
        shouldSend := true
        if throttleLevel > ThrottleNone {
            // Use message sequence number for deterministic throttling
            // This ensures consistent message delivery patterns
            msgSeq := client.seqGen.Next()

            switch throttleLevel {
            case ThrottleLight:
                shouldSend = msgSeq % 4 != 0 // Send 75%
            case ThrottleMedium:
                shouldSend = msgSeq % 2 == 0 // Send 50%
            case ThrottleHeavy:
                shouldSend = msgSeq % 4 == 0 // Send 25%
            }
        }

        if !shouldSend {
            // Skip this message but don't disconnect
            IncrementThrottledMessages()
            return true
        }

        // Attempt send with existing logic
        select {
        case client.send <- message:
            // Success - check if we can reduce throttling
            atomic.StoreInt32(&client.sendAttempts, 0)
            s.maybeReduceThrottle(client)

        default:
            // Buffer full - increase throttling instead of disconnecting
            attempts := atomic.AddInt32(&client.sendAttempts, 1)

            if attempts >= 3 {
                s.increaseThrottle(client)
                atomic.StoreInt32(&client.sendAttempts, 0) // Reset counter
            }
        }

        return true
    })
}

func (s *Server) increaseThrottle(client *Client) {
    level := atomic.LoadInt32(&client.throttleLevel)

    if level < ThrottleHeavy {
        newLevel := level + 1
        atomic.StoreInt32(&client.throttleLevel, newLevel)
        client.lastThrottleTime = time.Now()

        s.auditLogger.Warning("ClientThrottled", "Increased throttle level for slow client", map[string]interface{}{
            "clientID":      client.id,
            "throttleLevel": newLevel,
            "ratePercent":   s.getThrottleRate(newLevel),
        })

        // Send notification to client
        s.sendThrottleNotification(client, newLevel)
    } else {
        // Already at maximum throttle - disconnect as last resort
        s.auditLogger.Warning("SlowClientDisconnected", "Client too slow even at maximum throttle", map[string]interface{}{
            "clientID":      client.id,
            "throttleLevel": level,
        })

        conn := client.conn
        if conn != nil {
            closeMsg := ws.NewCloseFrameBody(ws.StatusPolicyViolation, "Client too slow even at reduced rate")
            ws.WriteFrame(conn, ws.NewCloseFrame(closeMsg))
            conn.Close()
        }

        IncrementSlowClientDisconnects()
    }
}

func (s *Server) maybeReduceThrottle(client *Client) {
    level := atomic.LoadInt32(&client.throttleLevel)

    if level == ThrottleNone {
        return // Already at full rate
    }

    // Reduce throttle after 30 seconds of successful sends
    if time.Since(client.lastThrottleTime) > 30*time.Second {
        newLevel := level - 1
        atomic.StoreInt32(&client.throttleLevel, newLevel)
        client.lastThrottleTime = time.Now()

        s.auditLogger.Info("ClientThrottleReduced", "Reduced throttle level for recovering client", map[string]interface{}{
            "clientID":      client.id,
            "throttleLevel": newLevel,
            "ratePercent":   s.getThrottleRate(newLevel),
        })

        // Send notification to client
        s.sendThrottleNotification(client, newLevel)
    }
}

func (s *Server) sendThrottleNotification(client *Client, level int32) {
    notification := map[string]interface{}{
        "type":          "throttle_update",
        "level":         level,
        "rate_percent":  s.getThrottleRate(level),
        "message":       s.getThrottleMessage(level),
    }

    if data, err := json.Marshal(notification); err == nil {
        select {
        case client.send <- data:
            // Notification sent
        default:
            // Can't send notification, client buffer full
        }
    }
}

func (s *Server) getThrottleRate(level int32) float64 {
    switch level {
    case ThrottleLight:  return 75.0
    case ThrottleMedium: return 50.0
    case ThrottleHeavy:  return 25.0
    default:             return 100.0
    }
}

func (s *Server) getThrottleMessage(level int32) string {
    switch level {
    case ThrottleLight:
        return "Your connection is slightly slow. Receiving 75% of updates."
    case ThrottleMedium:
        return "Your connection is slow. Receiving 50% of updates. Consider refreshing."
    case ThrottleHeavy:
        return "Your connection is very slow. Receiving 25% of updates. Please reconnect."
    default:
        return "Connection quality restored. Receiving all updates."
    }
}
```

#### Client-Side Handling

```typescript
// In frontend WebSocket client
ws.onmessage = (event) => {
  const msg = JSON.parse(event.data);

  if (msg.type === 'throttle_update') {
    // Show UI notification
    showNotification({
      type: msg.level > 1 ? 'warning' : 'info',
      message: msg.message,
      ratePercent: msg.rate_percent
    });

    // Log for debugging
    console.log(`Throttle level: ${msg.level} (${msg.rate_percent}% rate)`);

    // Maybe suggest reconnection
    if (msg.level >= 3) {
      showReconnectSuggestion();
    }
  }
};
```

#### Benefits

1. **Better User Experience**
   - Clients stay connected during temporary issues
   - Gradual degradation instead of hard disconnect
   - Clear feedback to users about connection quality

2. **Resource Efficiency**
   - Reduces reconnection storms
   - Slow clients use less bandwidth automatically
   - Fast clients unaffected

3. **Fairness**
   - Prevents slow clients from wasting server resources
   - Fast clients get 100% of messages
   - Automatic recovery when client speed improves

4. **Observability**
   - New Prometheus metrics: `ws_throttled_clients_by_level`
   - Track throttle transitions
   - Identify clients with persistent issues

#### Monitoring Metrics

```go
// Add to metrics.go
var (
    throttledClientsLight = promauto.NewGauge(prometheus.GaugeOpts{
        Name: "ws_throttled_clients_light",
        Help: "Number of clients at light throttle (75% rate)",
    })

    throttledClientsMedium = promauto.NewGauge(prometheus.GaugeOpts{
        Name: "ws_throttled_clients_medium",
        Help: "Number of clients at medium throttle (50% rate)",
    })

    throttledClientsHeavy = promauto.NewGauge(prometheus.GaugeOpts{
        Name: "ws_throttled_clients_heavy",
        Help: "Number of clients at heavy throttle (25% rate)",
    })

    throttleTransitions = promauto.NewCounterVec(prometheus.CounterOpts{
        Name: "ws_throttle_transitions_total",
        Help: "Number of throttle level transitions",
    }, []string{"from_level", "to_level"})
)
```

#### Implementation Effort

| Task | Effort | Priority |
|------|--------|----------|
| Core throttling logic | 4-6 hours | High |
| Client notifications | 2 hours | Medium |
| Prometheus metrics | 2 hours | Medium |
| Frontend UI handling | 3-4 hours | Medium |
| Testing & tuning | 4-6 hours | High |
| **Total** | **15-20 hours** | **Phase 2** |

#### Rollout Strategy

1. **Phase 1**: Implement with conservative defaults
   - Enable for 10% of users
   - Monitor metrics (throttle rates, disconnections)
   - Tune thresholds based on data

2. **Phase 2**: Gradual rollout
   - 25% â†’ 50% â†’ 100% of users
   - A/B test: throttling vs disconnection
   - Compare user retention metrics

3. **Phase 3**: Optimization
   - Tune throttle levels based on network type (4G, 5G, WiFi)
   - Consider geo-based thresholds
   - Machine learning for adaptive thresholds

#### Configuration

Add environment variables for tuning:

```yaml
# docker-compose.yml
environment:
  WS_THROTTLE_ENABLED: "true"
  WS_THROTTLE_LIGHT_PERCENT: "75"
  WS_THROTTLE_MEDIUM_PERCENT: "50"
  WS_THROTTLE_HEAVY_PERCENT: "25"
  WS_THROTTLE_RECOVERY_SECONDS: "30"
  WS_THROTTLE_MAX_LEVEL: "3"
```

#### Alternative: Priority-Based Throttling

For advanced use cases, implement priority-based message filtering:

```go
type MessagePriority int

const (
    PriorityLow    MessagePriority = 0 // Metadata updates
    PriorityMedium MessagePriority = 1 // Comment updates
    PriorityHigh   MessagePriority = 2 // Price updates
    PriorityCritical MessagePriority = 3 // Trade confirmations
)

// When throttled, drop low-priority messages first
func (s *Server) shouldSendMessage(client *Client, priority MessagePriority) bool {
    level := atomic.LoadInt32(&client.throttleLevel)

    switch level {
    case ThrottleLight:
        return priority >= PriorityMedium // Drop low priority
    case ThrottleMedium:
        return priority >= PriorityHigh // Drop low+medium
    case ThrottleHeavy:
        return priority == PriorityCritical // Only critical
    default:
        return true // Send all
    }
}
```

This ensures critical updates (like trade confirmations) always get through, even when throttled.

---

## Load Testing Lessons Learned

### Critical Issue: File Descriptor Limits

**Discovery Date**: October 12, 2025 during 10K capacity testing

**Problem**: Test plateau at 8,651 connections (86.5% success rate) instead of target 10K

**Root Cause**: Docker container file descriptor limit of **1,024** (default `ulimit -n`)
- Each WebSocket connection requires 1 file descriptor
- 8,651 connections â‰ˆ exhausted all available file descriptors
- Connection attempts beyond limit fail with `EMFILE` (too many open files)

**Impact**:
- **1,349 connection failures** (13.5% failure rate)
- Cannot scale beyond ~1,000 connections without ulimit configuration
- Same issue affects ANY high-connection service (not just WebSocket)

**Solution**: Add ulimit to Docker run command

```bash
# WRONG (default limit: 1,024)
docker run ws-test-runner

# CORRECT (support 10K+ connections)
docker run --ulimit nofile=200000:200000 ws-test-runner
```

**In docker-compose.yml**:
```yaml
services:
  ws-go:
    ulimits:
      nofile:
        soft: 200000
        hard: 200000
```

**In Taskfile for remote tests**:
```yaml
test:remote:capacity:
  cmds:
    - |
      gcloud compute ssh test-runner --command='docker run --rm \
        --ulimit nofile=200000:200000 \
        -e TARGET_CONNECTIONS=10000 \
        ws-test-runner'
```

**Verification**:
```bash
# Check current limit
ulimit -n

# Check inside container
docker exec <container> sh -c "ulimit -n"

# Should return: 200000 (not 1024)
```

**Why 200,000?**
- Linux default: 1,024 (too low)
- ws-go needs: 10K connections Ã— 2 FDs per connection = 20K
- Test client needs: 10K connections Ã— 1 FD = 10K
- Overhead: logging, metrics, health checks = ~100
- Safety margin: 10x = **200,000** recommended

**Related GCP Configuration**:
```bash
# Host-level limits (already configured in startup script)
echo "* soft nofile 200000" >> /etc/security/limits.conf
echo "* hard nofile 200000" >> /etc/security/limits.conf
```

### TCP SYN Queue Exhaustion

**Problem**: High connection ramp rates (100-200 conn/sec) can exhaust TCP SYN queue

**Symptoms**:
- Connection timeouts during ramp-up phase
- `ETIMEDOUT` errors in client logs
- Server CPU idle but connections failing
- Higher failure rate at 60-90 seconds (mid-ramp)

**Root Cause**: Linux kernel TCP SYN queue size limits
- Default `net.ipv4.tcp_max_syn_backlog = 1024`
- Default `net.core.somaxconn = 4096`
- When SYN queue full, kernel drops new connection attempts silently

**Solution**: Increase kernel TCP parameters

```bash
# On ws-go server (via startup script or sysctl.conf)
sysctl -w net.ipv4.tcp_max_syn_backlog=8192
sysctl -w net.core.somaxconn=8192
sysctl -w net.ipv4.tcp_syncookies=1  # Enable SYN cookies

# Make persistent
echo "net.ipv4.tcp_max_syn_backlog = 8192" >> /etc/sysctl.conf
echo "net.core.somaxconn = 8192" >> /etc/sysctl.conf
echo "net.ipv4.tcp_syncookies = 1" >> /etc/sysctl.conf
```

**Application-Level Mitigation**:
```go
// In server.go - increase listen backlog
listener, err := net.Listen("tcp", ":3004")
// Default backlog too low for high connection rates

// Better: explicitly set backlog
lc := net.ListenConfig{
    Control: func(network, address string, c syscall.RawConn) error {
        return c.Control(func(fd uintptr) {
            syscall.SetsockoptInt(int(fd), syscall.SOL_SOCKET, syscall.SO_REUSEADDR, 1)
            // Backlog = somaxconn (8192)
        })
    },
}
listener, err := lc.Listen(context.Background(), "tcp", ":3004")
```

**Test Client Mitigation**:
- Reduce ramp rate: 100 conn/sec â†’ 50 conn/sec (doubles ramp time but more stable)
- Add connection retry logic with exponential backoff
- Spread connections across multiple source IPs (if available)

**Monitoring**:
```bash
# Check SYN queue drops
netstat -s | grep -i "SYNs to LISTEN"
# Should show 0 SYN drops

# Watch connection states
watch -n 1 'ss -s'
# Monitor SYN-RECV count (should stay low)
```

**Impact of Not Fixing**:
- 10K test: ~5-10% connection failure rate
- 18K test: ~15-20% connection failure rate
- Production: Intermittent connection failures during traffic spikes

**Recommendation**:
1. **Mandatory** for production deployment
2. Test after fixing: should see <1% connection failure rate
3. Include in GCP instance startup script

### Test Client vs Server Resource Requirements

**Discovery**: Test client has DIFFERENT resource needs than server

| Resource | ws-go Server (per conn) | Test Client (per conn) |
|----------|------------------------|------------------------|
| Memory | 0.7MB (700KB) | 0.2MB (200KB) |
| File descriptors | 2 (read + write) | 1 (socket only) |
| Goroutines | 2 (readPump + writePump) | 0 (Node.js event loop) |
| CPU | ~0.003% (with filtering) | ~0.005% (message processing) |

**Implication**: 10K connection test requires:
- **Server**: 7GB RAM, 2 vCPU â†’ e2-standard-2 (8GB)
- **Test client**: 2GB RAM, 1-2 vCPU â†’ e2-standard-4 (16GB) for safety

**Why test-runner needs MORE RAM than server?**
- Node.js heap overhead: ~500MB base
- WebSocket client library buffers: larger than server buffers
- Message processing: JSON parsing, statistics tracking
- Logging overhead: more verbose than server

**Lesson**: Always size test infrastructure independently from server capacity planning

### Critical Issue: Ephemeral Port Exhaustion on Test Clients

**Discovery Date**: October 12, 2025 during 10K capacity testing (after file descriptor fix)

**Problem**: Test plateau at ~8,300 connections (83-84% success rate) with **1,575 "UNKNOWN" failures**
- All failures occur during ramp-up phase (60s-100s)
- Server healthy: 0 rejections, 56-60% CPU, 18.5% memory
- File descriptor limit already raised to 200K

**Root Cause**: Test-runner exhausting **Linux ephemeral port range**
- Default range: 32768-60999 = **28,231 available ports**
- Each outbound connection needs a unique source port
- Ports in TIME_WAIT state (60s) cannot be reused
- At 100 conn/sec ramp rate: old connections still in TIME_WAIT
- Result: Run out of ports around 8-10K connections

**Why This Happens**:
```
Test client makes 10K connections from single IP to same destination (10.128.0.10:3004)
â”œâ”€ Connection 1: 10.128.0.11:32768 â†’ 10.128.0.10:3004
â”œâ”€ Connection 2: 10.128.0.11:32769 â†’ 10.128.0.10:3004
â”œâ”€ ...
â””â”€ Connection 8,300: 10.128.0.11:60999 â†’ 10.128.0.10:3004 âŒ Out of ports!
```

**Diagnosis Commands**:
```bash
# Check ephemeral port range
cat /proc/sys/net/ipv4/ip_local_port_range
# Default: 32768  60999 (28,231 ports)

# Check TIME_WAIT connections
ss -s
# Look for high count of TIME_WAIT sockets

# Check connection tracking
sysctl net.netfilter.nf_conntrack_max net.netfilter.nf_conntrack_count
```

**Solution**: TCP tuning on test-runner

```bash
# Apply immediately (test-runner instance)
gcloud compute ssh odin-test-runner --command='
  # Increase ephemeral port range (2.3x more ports)
  sudo sysctl -w net.ipv4.ip_local_port_range="1024 65535"

  # Enable TIME_WAIT socket reuse (CRITICAL!)
  sudo sysctl -w net.ipv4.tcp_tw_reuse=1

  # Reduce TIME_WAIT timeout (4x faster cleanup)
  sudo sysctl -w net.ipv4.tcp_fin_timeout=15

  # Increase connection tracking table
  sudo sysctl -w net.netfilter.nf_conntrack_max=200000

  # Make persistent across reboots
  sudo tee -a /etc/sysctl.conf <<EOF
net.ipv4.ip_local_port_range = 1024 65535
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_fin_timeout = 15
net.netfilter.nf_conntrack_max = 200000
EOF
'
```

**Impact of Fix**:

| Parameter | Before | After | Impact |
|-----------|--------|-------|--------|
| **Ephemeral Port Range** | 32768-60999 (28K) | 1024-65535 (64K) | 2.3x more ports |
| **tcp_tw_reuse** | Disabled (2) | Enabled (1) | Reuse TIME_WAIT sockets |
| **tcp_fin_timeout** | 60s | 15s | 4x faster cleanup |
| **nf_conntrack_max** | Default (~65K) | 200K | Prevents tracking exhaustion |

**Expected Results**:
- **Before**: Plateaued at ~8.3K connections (1,575 failures = 15.75% failure rate)
- **After**: Should reach **9.5K-10K connections** with <5% failure rate

**Why tcp_tw_reuse is Safe**:
- Only reuses TIME_WAIT sockets for **outbound connections**
- Safe for clients making many connections to same destination
- NOT recommended for servers (can cause issues with load balancers)
- Perfect for load testing scenarios

**When This Matters**:
1. **Load testing**: Any test with >8K connections from single client
2. **High-traffic clients**: Services making many outbound connections (proxies, aggregators)
3. **Connection pooling**: Applications with high connection churn

**Does NOT affect**:
- **WebSocket server**: Inbound connections don't use ephemeral ports
- **Production clients**: Browsers rarely make >100 concurrent connections

**GCP Instance Startup Script** (add to test-runner only):
```bash
#!/bin/bash
# TCP tuning for high connection count load testing
cat >> /etc/sysctl.conf <<EOF
net.ipv4.ip_local_port_range = 1024 65535
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_fin_timeout = 15
net.netfilter.nf_conntrack_max = 200000
EOF

sysctl -p
```

**Verification**:
```bash
# Before test
sysctl net.ipv4.ip_local_port_range
# Should show: 1024    65535

sysctl net.ipv4.tcp_tw_reuse
# Should show: net.ipv4.tcp_tw_reuse = 1

# During test - watch TIME_WAIT count
watch -n 2 'ss -s | grep -i time-wait'
# Should stay reasonable (<10K) even at 10K connections

# After test - check for port exhaustion
dmesg | grep -i "out of"
# Should show no port exhaustion errors
```

**Related Issues**:
- File descriptor limits (already documented above)
- Connection tracking table exhaustion (nf_conntrack_max)
- TCP SYN queue (already documented above)

**Lesson Learned**:
**Load test infrastructure needs TCP tuning separate from application servers**. Production WebSocket servers don't need these settings (inbound connections only), but load test clients making 10K+ outbound connections absolutely do.

### Critical Bottleneck: Memory Capacity Limits (The Real Ceiling)

**Discovery Date**: October 12, 2025 during 10K capacity testing (after TCP tuning applied)

**Problem**: Test plateaus at **8,849 connections (88.5% success rate)** despite TCP tuning
- 1,151 "UNKNOWN" failures (down from 1,575 pre-TCP-fix)
- Server healthy: 0 rejections, 60% CPU, but memory at **7GB limit**
- TCP tuning gave +524 connections (+6.2%), then hit hard ceiling

**Root Cause**: Container **memory capacity exhaustion** at 7GB limit
- Memory per connection: **0.7 MB** (measured)
- Container limit: **7,168 MB** (7GB)
- Theoretical max: 7,168 Ã· 0.7 = **10,240 connections**
- Actual achieved: **8,849 connections** (86.5% of theoretical)
- **Missing overhead**: ~974 MB (base process, NATS, workers, goroutines)

**The Math Breakdown**:
```
Connection memory footprint (per connection):
â”œâ”€ Connection struct: ~50 KB
â”œâ”€ Read buffer (64 KB): 64 KB
â”œâ”€ Write buffer (64 KB): 64 KB
â”œâ”€ Send channel (256 msgs Ã— 1KB): 256 KB
â”œâ”€ Replay buffer (100 msgs Ã— 1KB): 100 KB
â”œâ”€ Goroutine stacks (2 Ã— 8KB): 16 KB
â”œâ”€ Subscription map: ~10 KB
â””â”€ Misc overhead: ~140 KB
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Total per connection: ~700 KB (0.7 MB)

Fixed overhead (independent of connections):
â”œâ”€ Base server process: 200-300 MB
â”œâ”€ NATS consumer buffers: 100-200 MB
â”œâ”€ Worker pool (256 Ã— 108KB): 28 MB
â”œâ”€ Goroutine stacks (system): 144 MB
â”œâ”€ Message buffers in flight: 200-300 MB
â””â”€ Docker/OS overhead: 100-200 MB
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Total fixed overhead: 772-1,172 MB

Actual memory usage at 8,849 connections:
Connections: 8,849 Ã— 0.7 MB = 6,194 MB
Overhead: ~974 MB (matches range above âœ“)
Total: 7,168 MB (exactly at limit âœ“)
```

**Why Failures Accelerate Near Memory Limit**:

Failure distribution over time shows clear memory pressure pattern:
```
Time Range    Connections    Failures    Failure Rate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0-60s         0 â†’ 5,315      56          1.0%  (plenty of memory)
60-80s        5,315 â†’ 6,880  491         7.5%  (memory filling)
80-100s       6,880 â†’ 8,257  568         12.5% (CRITICAL: near limit)
100-106s      8,257 â†’ 8,849  27          4.5%  (plateau reached)
```

**What happens in the critical zone (80-100s)**:
1. Memory usage: 6.5GB â†’ 7GB (approaching limit)
2. Go GC pressure increases (more frequent collections)
3. GC pause times increase (10ms â†’ 50-100ms)
4. WebSocket upgrade handshakes slow down
5. Client connection timeouts trigger â†’ "UNKNOWN" errors
6. Server never gets to reject (timeout during upgrade)

**Grafana Evidence**:

**Memory Usage Graph** (critical insight):
- Green line (used): Climbs steadily to ~6.8-7GB
- Yellow line (limit): Flat at 7,168 MB (7GB)
- Pattern: Reaches limit, plateaus, can't grow further
- After test: Sharp drop (GC reclaims all connection memory)

**CPU Usage** (NOT the bottleneck):
- 60% during ramp-up (healthy)
- Plenty of headroom (40% available)
- Drops to ~0% after test completes

**Goroutines** (NOT the bottleneck):
- 18K active goroutines during test
- Formula: `(8,849 Ã— 2) + 256 + system = 17,954`
- Well within limit: 18K / 25K (72% utilization)

**Connections Over Time Graph**:
- Steep ramp to 8K (good progress)
- Hard plateau at 8.8K (flat line = hitting limit)
- Cannot grow further despite connection attempts

**Why Server Reports 0 Rejections**:

This confused us initially - server shows 0 rejections, yet 1,151 failures occurred.

**What's actually happening**:
1. Client initiates TCP handshake â†’ **Success** (3-way handshake completes)
2. Client sends WebSocket upgrade request â†’ Server receives it
3. **Server processing slows** due to GC pressure and memory allocation delays
4. Server tries to allocate memory for new connection â†’ **Slow** (near limit)
5. Client times out waiting for upgrade response (30-60s Node.js default)
6. Client reports "UNKNOWN" error and closes connection
7. **Server never gets to reject** - connection dies during handshake

Server's perspective: "I was trying to accept the connection, but client disappeared"
Client's perspective: "Server too slow, giving up"

Result: **Timeout, not rejection** â†’ 0 in server metrics but failure on client side.

**Verification Commands**:

```bash
# During test - watch memory usage real-time
gcloud compute ssh odin-ws-go --command='
  watch -n 1 "docker stats --no-stream | grep odin-ws-go"
'

# Expected output at plateau:
# NAME         CPU %    MEM USAGE / LIMIT      MEM %
# odin-ws-go   60.0%    6.95GiB / 7.0GiB       99.3%

# Check GC pressure
gcloud compute ssh odin-ws-go --command='
  docker logs odin-ws-go 2>&1 | grep -i "gc pause"
'
# High GC pause times (>50ms) indicate memory pressure

# Verify no memory leaks after test
gcloud compute ssh odin-ws-go --command='
  docker stats --no-stream odin-ws-go
'
# Memory should drop to baseline (~200-300MB) after connections close
```

**Three Solutions (Pick Your Poison)**:

### Solution A: Accept 8.8K Capacity âœ… RECOMMENDED FOR NOW

**What you get**:
- 8,849 concurrent connections (88.5% success rate)
- Server stable and healthy (60% CPU, 0 actual rejections)
- Proven capacity on e2-standard-2 (8GB RAM)
- No additional cost or code changes

**When this is enough**:
- Business needs â‰¤8,800 concurrent users per instance
- Scale horizontally: Deploy 2Ã— instances with load balancer = 17.6K capacity
- Cost: $24 Ã— 2 = $48/month (same as upgrading to e2-standard-4)
- Benefit: High availability, zero-downtime deployments

**Verdict**: **Excellent result for the hardware.** This validates the architecture scales linearly with memory.

### Solution B: Optimize Memory Per Connection ðŸ”§ MEDIUM EFFORT

**Goal**: Reduce **0.7MB/conn â†’ 0.4MB/conn** (43% reduction)

**Optimizations**:

1. **Reduce replay buffer size** (`connection.go`):
   ```go
   // BEFORE:
   const replayBufferSize = 100  // 100KB per connection

   // AFTER:
   const replayBufferSize = 50   // 50KB per connection
   ```
   - Savings: **50KB per connection**
   - Impact: Still supports 50-message replay (adequate for 30s @ 100msg/min)

2. **Reduce send channel size** (`connection.go`):
   ```go
   // BEFORE:
   send: make(chan []byte, 256),  // 256KB buffer

   // AFTER:
   send: make(chan []byte, 128),  // 128KB buffer
   ```
   - Savings: **128KB per connection**
   - Impact: Still handles bursts (1.28s buffer @ 100msg/s)

3. **Use sync.Pool for temporary buffers** (`server.go`):
   ```go
   var messagePool = sync.Pool{
       New: func() interface{} {
           return make([]byte, 0, 1024)
       },
   }

   // In broadcast function:
   buf := messagePool.Get().([]byte)
   defer messagePool.Put(buf)
   ```
   - Savings: **~50KB per connection** (reduced allocation overhead)

4. **Optimize subscription map** (`connection.go`):
   ```go
   // BEFORE: map[string]bool (40 bytes per entry + overhead)
   // AFTER: Use bitset or slice for common patterns

   subscriptions []string  // Slice instead of map for small counts
   ```
   - Savings: **~20KB per connection** (most clients subscribe to 1-5 channels)

**Expected Result**:
```
Memory per connection: 0.7 MB â†’ 0.4 MB (43% reduction)
Max connections on 7GB: 7,168 Ã· 0.4 = 17,920 connections (~18K)

Capacity improvement: 8,849 â†’ 17,920 (+102% increase)
```

**Effort**: 2-3 days (code changes + comprehensive testing)
**Cost**: $0 (code changes only)
**Risk**: Medium (need thorough testing for edge cases)

**When to do this**:
- Need >10K connections per instance
- Want to stay on e2-standard-2 ($24/month)
- Have engineering time for optimization

### Solution C: Scale Up Hardware ðŸ’° INSTANT FIX

**Upgrade to e2-standard-4**:
- RAM: 8GB â†’ **16GB** (2x)
- vCPU: 2 â†’ **4** (2x)
- Network: 2 Gbps â†’ **4 Gbps** (2x)
- Cost: $24/month â†’ **$72/month** (+$48)

**Configuration changes** (`isolated/ws-go/docker-compose.yml`):
```yaml
environment:
  # BEFORE:
  - WS_MEMORY_LIMIT=7516192768      # 7 GB
  - WS_MAX_CONNECTIONS=10000
  - WS_WORKER_POOL_SIZE=256
  - WS_MAX_GOROUTINES=25000

  # AFTER:
  - WS_MEMORY_LIMIT=15032385536     # 14 GB (leave 2GB for OS)
  - WS_MAX_CONNECTIONS=20000
  - WS_WORKER_POOL_SIZE=512          # max(32, 20000/40) = 500 â†’ 512
  - WS_MAX_GOROUTINES=50000          # ((20000Ã—2) + 512 + 13) Ã— 1.2 = 48,630

deploy:
  resources:
    limits:
      memory: 14336M    # 14 GB
      cpus: "3.9"       # 95% of 4 vCPU
```

**Expected Result**:
```
Available memory: 14,336 MB
Memory per connection: 0.7 MB
Max connections: 14,336 Ã· 0.7 = 20,480 connections (~20K)

Capacity improvement: 8,849 â†’ 20,480 (+131% increase)
```

**Effort**: 2 hours (instance recreation + config updates + testing)
**Cost**: +$48/month (3x current cost)
**Risk**: Low (simple hardware upgrade, no code changes)

**When to do this**:
- Need 15K-20K connections per instance immediately
- No time for code optimization
- Budget allows for 3x cost increase
- Want instant capacity without engineering effort

**Taskfile changes** (`taskfiles/isolated-setup.yml`):
```yaml
# Line 28: Update machine type
WS_GO_MACHINE_TYPE: e2-standard-4  # Was: e2-standard-2
```

---

## Comparison Matrix

| Metric | Current (8GB) | Option A: Accept | Option B: Optimize | Option C: Scale Up |
|--------|--------------|------------------|--------------------|--------------------|
| **Max Connections** | 8,849 | 8,849 | ~18,000 | ~20,000 |
| **Success Rate** | 88.5% | 88.5% | 95%+ | 95%+ |
| **Cost per Instance** | $24/month | $24/month | $24/month | $72/month |
| **Engineering Effort** | N/A | 0 hours | 16-24 hours | 2 hours |
| **Risk** | N/A | None | Medium | Low |
| **Time to Deploy** | N/A | Immediate | 3-5 days | 2 hours |
| **HA Strategy** | 2 instances | 2 instances | 2 instances | 1 instance |
| **Total Capacity (HA)** | 17.6K | 17.6K | 36K | 20K |
| **Total Cost (HA)** | $48 | $48 | $48 | $72 |

---

## Recommended Strategy (Phased Approach)

### Phase 1: Short-term (This Week) - **Option A**
Accept 8.8K capacity, validate business needs:
- Deploy 2Ã— e2-standard-2 instances with load balancer
- Total capacity: **17.6K concurrent connections**
- Cost: **$48/month** (+ $18 for load balancer)
- Monitor actual production usage

### Phase 2: Medium-term (Next Month) - **Option B** (if needed)
If production data shows need for higher per-instance capacity:
- Optimize memory per connection (0.7MB â†’ 0.4MB)
- Target: **18K per instance**
- Scale to 2Ã— instances = **36K total capacity**
- Same cost: **$48/month**

### Phase 3: Long-term (3-6 months) - Horizontal Scaling
As user base grows:
- Add instances dynamically with GCP Managed Instance Groups
- Auto-scaling based on connection count
- Max tested capacity per instance: 18K (after optimization)

---

## Key Lessons Learned

1. **Memory is the ultimate bottleneck** for WebSocket servers
   - Not CPU, not network, not file descriptors
   - Linear relationship: Capacity = Available Memory Ã· Memory Per Connection
   - Measure memory footprint early in testing

2. **88.5% success rate at capacity is normal**
   - Near-limit GC pressure causes slowdowns
   - Timeouts during handshake â‰  rejections
   - Don't chase 100% - not realistic at hardware limits

3. **Server metrics can be misleading**
   - 0 rejections â‰  0 failures
   - Client timeouts don't show up as rejections
   - Always correlate client-side and server-side metrics

4. **TCP tuning was necessary but not sufficient**
   - Fixed ephemeral port exhaustion (+524 connections)
   - Revealed the true bottleneck (memory)
   - Multi-layered bottlenecks require iterative debugging

5. **Hardware vs. optimization trade-off**
   - Optimization: 2-3 days effort, 0 cost, 2x capacity
   - Hardware: 2 hours effort, 3x cost, 2.3x capacity
   - Choose based on: timeline, budget, engineering capacity

6. **Horizontal scaling beats vertical scaling**
   - 2Ã— e2-standard-2 ($48) = same cost as 1Ã— e2-standard-4 ($72)
   - 2Ã— instances = high availability built-in
   - 2Ã— instances = zero-downtime deployments possible

---

**Lesson Learned**:
**WebSocket capacity is fundamentally memory-bound, not CPU-bound**. At scale, every connection needs ~0.7MB of memory for buffers, queues, and goroutines. Plan capacity based on available RAM, not CPU cores. For production, horizontal scaling (multiple smaller instances) provides better cost-efficiency and reliability than vertical scaling (one large instance).

---

## Next Steps

1. **This week**: Sign up for Synadia Cloud (5 min)
2. **Week 1**: Deploy infrastructure (ws-go + monitoring)
3. **Week 2**: Code changes (subscriptions) + backend integration
4. **Week 3**: Frontend integration
5. **Week 4**: Testing
6. **Week 5-6**: Rollout

**Questions before starting**:
1. Do you have GCP project set up?
2. Who will handle backend integration (publish to NATS)?
3. Who will handle frontend integration (React components)?
4. What's your target launch date?

Ready to start? ðŸš€
