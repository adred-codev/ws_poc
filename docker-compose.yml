services:
  # NATS Message Broker
  nats:
    image: nats:2.12-alpine
    container_name: odin-nats
    ports:
      - "4222:4222" # Client connections
      - "8222:8222" # HTTP monitoring
      - "6222:6222" # Cluster routing
    command:
      - "--jetstream"
      - "--store_dir=/data"
      - "--http_port=8222"
    volumes:
      - nats_data:/data
    networks:
      - odin-network
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:8222/healthz",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: "0.25"              # Reduced for ws-server priority
          memory: 128M
        reservations:
          cpus: "0.25"
          memory: 128M

  # Go WebSocket Server (Production)
  ws-go:
    build:
      context: ./src
      dockerfile: Dockerfile
    container_name: odin-ws-go
    ports:
      - "3004:3002" # WebSocket + Metrics port
    command:
      ["./odin-ws-server", "-addr", ":3002", "-nats", "nats://nats:4222"]
    environment:
      # Static resource configuration (explicit limits)
      - WS_CPU_LIMIT=1.5           # Must match deploy.resources.limits.cpus below
      - WS_MEMORY_LIMIT=805306368  # 768MB in bytes
      - WS_MAX_CONNECTIONS=500
      # Worker pool sizing:
      #   Default: cpuLimit*2 = 1.5*2 = 3 workers (CPU-bound workload)
      #   Override: 32 workers (10x default for I/O-bound broadcast fanout)
      #   Reason: Workers spend most time blocked on network writes, not CPU
      #   Tradeoff: More workers = better broadcast throughput, more context switching
      - WS_WORKER_POOL_SIZE=32
      - WS_WORKER_QUEUE_SIZE=3200  # 100x worker count
      # Rate limiting (prevent overload)
      - WS_MAX_NATS_RATE=20        # Max NATS messages/sec
      - WS_MAX_BROADCAST_RATE=20   # Max broadcasts/sec
      - WS_MAX_GOROUTINES=1000     # Hard goroutine limit
      # Safety thresholds
      - WS_CPU_REJECT_THRESHOLD=75.0
      - WS_CPU_PAUSE_THRESHOLD=80.0
      # Logging
      - LOG_LEVEL=info             # debug, info, warn, error
      - LOG_FORMAT=json            # json for Loki
    networks:
      - odin-network
    depends_on:
      nats:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "1.5"               # 75% of e2-micro
          memory: 768M              # 75% of RAM
          pids: 1000
        reservations:
          cpus: "1.5"
          memory: 768M
    ulimits:
      nofile:
        soft: 200000
        hard: 200000
      nproc:
        soft: 32768
        hard: 32768

  # NATS Publisher Service
  publisher:
    build:
      context: ./publisher
      dockerfile: Dockerfile
    container_name: odin-publisher
    ports:
      - "3003:3003" # HTTP control API
    environment:
      - NATS_URL=nats://nats:4222
      - PORT=3003
      - NODE_ENV=production
      - TOKENS=BTC,ETH,ODIN,SOL,DOGE
    networks:
      - odin-network
    depends_on:
      nats:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M
        reservations:
          cpus: "0.1"
          memory: 64M

  # Prometheus (Metrics Collection)
  prometheus:
    image: prom/prometheus:v3.6.0
    container_name: odin-prometheus
    ports:
      - "9091:9090" # Prometheus UI
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'  # 7d retention (e2-small)
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    networks:
      - odin-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.15"
          memory: 128M          # Increased from 64M for e2-small
        reservations:
          cpus: "0.15"
          memory: 128M

  # Grafana (Visualization)
  grafana:
    image: grafana/grafana:12.2.0
    container_name: odin-grafana
    ports:
      - "3010:3000" # Grafana UI
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    networks:
      - odin-network
    depends_on:
      - prometheus
      - loki
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 128M          # Increased from 32M (was OOM killed at 92MB)
        reservations:
          cpus: "0.1"
          memory: 128M

  # Loki (Log Aggregation)
  loki:
    image: grafana/loki:3.3.2
    container_name: odin-loki
    ports:
      - "3101:3100" # Loki API
    volumes:
      - ./loki-config.yml:/etc/loki/loki-config.yml
      - loki_data:/loki
    command: -config.file=/etc/loki/loki-config.yml
    networks:
      - odin-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 128M          # Increased from 32M (was OOM killed at 96MB)
        reservations:
          cpus: "0.1"
          memory: 128M

  # Promtail (Log Collector)
  promtail:
    image: grafana/promtail:3.3.2
    container_name: odin-promtail
    volumes:
      - ./promtail-config.yml:/etc/promtail/promtail-config.yml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: -config.file=/etc/promtail/promtail-config.yml
    networks:
      - odin-network
    depends_on:
      - loki
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: 64M           # Increased from 16M (was OOM killed at 71MB)
        reservations:
          cpus: "0.1"
          memory: 64M

networks:
  odin-network:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: 1450

volumes:
  nats_data:
  prometheus_data:
  grafana_data:
  loki_data:
