# =============================================================================
# GCP Production Overrides - MULTI-CORE MODE
# =============================================================================
# Purpose: Override shared/base.env for GCP multi-core production deployment
# Load order: base.env → kafka-topics.env → ports.env → overrides.multi.env → .env.production
# Instance: e2-standard-4 (4 vCPU, 16GB RAM)
# Target: 12,000+ connections distributed across 3 shards (4K per shard)
#
# Architecture Notes:
#   - Each shard has its own Kafka consumer group (e.g., ws-server-production-0, -1, -2)
#   - BroadcastBus coordinates message distribution across all shards
#   - LoadBalancer uses "least connections" strategy for even distribution
#   - Per-shard max connections: WS_MAX_CONNECTIONS / num_shards

ENVIRONMENT=production-multi

# =============================================================================
# KAFKA CONNECTION
# =============================================================================
# Note: KAFKA_BROKERS is set in .env.production (uses BACKEND_INTERNAL_IP)
# Base consumer group name; shards append -0, -1, -2
KAFKA_GROUP_ID=ws-server-production  # Base name

# =============================================================================
# RESOURCE LIMITS (Multi-Core Distribution)
# =============================================================================
# Strategy: Test optimizations with 18K capacity (6K per shard × 3)
# Baseline: 5,710 connections @ 328% CPU (3 cores maxed)
# Target: 7,000-7,500 connections with optimizations (20-30% improvement)
# CPU: 3 cores total (1 core per shard, managed by Docker CPU limits)
# Memory: 14.5GB total (distributed across shards, ~6GB per shard at 18K)
WS_CPU_LIMIT=3                # Total CPU limit (Docker distributes)
WS_MEMORY_LIMIT=15569256448   # 14.5 GB (90% of 16GB) - same as single-core
WS_MAX_CONNECTIONS=18000      # Total connections (divided by shard count in code: 18K/3 = 6K per shard)

# =============================================================================
# RATE LIMITING (Production-validated)
# =============================================================================
WS_MAX_KAFKA_RATE=25         # Kafka message consumption rate per shard
WS_MAX_BROADCAST_RATE=25     # Broadcast rate limit per shard

# =============================================================================
# GOROUTINE LIMIT (Calculated for multi-core capacity)
# =============================================================================
# Formula per shard: ((4,000 × 2) + overhead) × 1.2 ≈ 10,000 per shard
# Total: 30,000 (matches single-core for compatibility)
WS_MAX_GOROUTINES=30000

# =============================================================================
# ⚠️  TEMPORARY OPTIMIZATION - TESTING PHASE
# =============================================================================
# Added: 2025-11-12
# Purpose: Reduce CPU measurement lag to improve admission control
# Revert if: Performance degrades or CPU measurement overhead becomes an issue
#
# TO REVERT: Comment out or remove METRICS_INTERVAL line below (will use default 15s)
#
# METRICS_INTERVAL: How often to measure CPU/memory for admission control
# Default: 15s (allows up to 1,500 connections between measurements at 100 conn/sec)
# Testing: 1s (allows max ~100 connections between measurements)
#
# Expected Impact:
#   - Faster reaction to CPU spikes (15× faster)
#   - Reduced overshoot after crossing 75% CPU threshold
#   - Target: 10-15% more successful connections before rejection
METRICS_INTERVAL=1s  # ⚠️ TEMPORARY - Testing faster CPU monitoring
