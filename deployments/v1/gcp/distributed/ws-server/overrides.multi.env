# =============================================================================
# GCP Production Overrides - MULTI-CORE MODE
# =============================================================================
# Purpose: Override shared/base.env for GCP multi-core production deployment
# Load order: base.env → kafka-topics.env → ports.env → overrides.multi.env → .env.production
# Instance: e2-highcpu-8 (8 vCPU, 8GB RAM)
# Target: 18,000+ connections distributed across 3 shards (6K per shard)
#
# Architecture Notes:
#   - Each shard has its own Kafka consumer group (e.g., ws-server-production-0 through -2)
#   - BroadcastBus coordinates message distribution across all shards
#   - LoadBalancer uses "least connections" strategy for even distribution
#   - Per-shard max connections: WS_MAX_CONNECTIONS / 3 shards (6,000 per shard)

ENVIRONMENT=production-multi

# =============================================================================
# KAFKA CONNECTION
# =============================================================================
# Note: KAFKA_BROKERS is set in .env.production (uses BACKEND_INTERNAL_IP)
# Base consumer group name; shards append -0 through -2 (3 shards total)
KAFKA_GROUP_ID=ws-server-production  # Base name

# =============================================================================
# SHARD CONFIGURATION (Multi-Core Architecture)
# =============================================================================
# Production: 3 shards optimal (empirically better than 7 shards - less overhead)
# Debug: Override in .env.debug (create .env.debug from .env.debug.template)
#
# To enable single-shard debug mode:
#   cp .env.debug.template .env.debug
#   docker-compose -f docker-compose.multi.yml up -d --force-recreate
#
# To return to production mode:
#   rm .env.debug
#   docker-compose -f docker-compose.multi.yml up -d --force-recreate
NUM_SHARDS=3              # Number of shard instances (3 optimal for e2-highcpu-8)
SHARD_BASE_PORT=3002      # First shard port (subsequent shards use 3003, 3004, etc.)
LB_PORT=3001              # LoadBalancer external port
LB_ADDR=:3001             # LoadBalancer bind address

# =============================================================================
# RESOURCE LIMITS (Multi-Core Distribution)
# =============================================================================
# Strategy: Optimal 3-shard configuration (empirically better than 7 shards)
# Reason: Fewer shards = less coordination overhead (Kafka, BroadcastBus, LoadBalancer)
# Previous 7 shards: 4,754/12K (39.6%, 701 conn/core) - too much overhead
# Current 3 shards: 5,180/12K (43.2%, 1,762 conn/core) - proven baseline
# CPU: 7 cores total (3 shards + system + headroom)
# Memory: 7GB total (85% of 8GB, plenty for 3 shards)
WS_CPU_LIMIT=7                # Total CPU limit (3 shards use ~3 cores, rest for system/headroom)
WS_MEMORY_LIMIT=7516192768    # 7 GB (85% of 8GB) - e2-highcpu-8 has less RAM
WS_MAX_CONNECTIONS=18000      # Total connections (divided by shard count: 18K/3 = 6,000 per shard)

# =============================================================================
# RATE LIMITING (Production-validated)
# =============================================================================
WS_MAX_KAFKA_RATE=25         # Kafka message consumption rate per shard
WS_MAX_BROADCAST_RATE=25     # Broadcast rate limit per shard

# =============================================================================
# GOROUTINE LIMIT (Calculated for multi-core capacity with LoadBalancer)
# =============================================================================
# HISTORY OF FIXES:
# - Original: 15,000 (failed at 999 connections)
# - First fix: 50,000 (failed at 10,690 connections - missing LoadBalancer goroutines!)
# - Second fix: 100,000 (accounts for full architecture)
#
# GOROUTINE SOURCES AT 18K CONNECTIONS:
# 1. WebSocket Shard Connections:
#    - Each connection = 2 goroutines (readPump + writePump)
#    - 18,000 connections × 2 = 36,000 goroutines
#
# 2. LoadBalancer Proxy (CRITICAL - was missing from previous calculation!):
#    - Each proxied connection = 2 goroutines (client→backend, backend→client)
#    - 18,000 connections × 2 = 36,000 goroutines
#
# 3. Worker Pools & Overhead:
#    - Kafka consumers: ~192 goroutines
#    - BroadcastBus workers: ~200 goroutines
#    - Monitoring: ~100 goroutines
#    - HTTP servers: ~100 goroutines
#    - Misc overhead: ~500 goroutines
#    - Total: ~1,500 goroutines
#
# TOTAL CALCULATION:
#   Shard goroutines:        36,000
#   LoadBalancer goroutines: 36,000
#   Workers + overhead:       1,500
#   ─────────────────────────────────
#   Base total:              73,500
#   Safety margin (×1.3):    95,550
#   Rounded:                100,000
#
# EVIDENCE FROM LOAD TESTING:
# - At 10,690 connections: 53,498 goroutines observed (limit: 50,000)
# - Calculation: 10,690 × 4 (2 shard + 2 proxy) + 1,500 = ~44,260 base
# - With overhead and safety: ~53,500 (matches observed!)
# - Extrapolated to 18K: 18,000 × 4 + 1,500 = ~73,500 base, ~95,000 with safety
#
WS_MAX_GOROUTINES=100000
